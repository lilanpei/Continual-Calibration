{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from avalanche.training.supervised import DER, Replay\n",
    "from avalanche.benchmarks.classic import SplitCIFAR100, SplitTinyImageNet\n",
    "from ResNet18 import resnet18\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from ECE_metrics import ExperienceECE, ExpECEHistogram\n",
    "from avalanche.evaluation.metrics import accuracy_metrics\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:3\"\n",
    "num_bins = 16\n",
    "# num_classes, benchmark = 100, SplitCIFAR100(n_experiences=10)\n",
    "num_classes, benchmark = 200, SplitTinyImageNet(n_experiences=10)\n",
    "test_stream = benchmark.test_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ResNet18\"\n",
    "model = resnet18(num_classes)\n",
    "# model_base_path = \"./logs/SplitCIFAR100_DER_2000_2/model_DER_NoSelfTraining_NoPostProcessing1\"\n",
    "# model_base_path = \"./logs/TinyImageNet_D2/model_DER_NoSelfTraining_NoPostProcessing2\"\n",
    "\n",
    "model_base_path = \"./logs/TinyImageNet_R2/model_Replay_NoSelfTraining_NoPostProcessing2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plugins = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    ExperienceECE(num_bins=num_bins),\n",
    "    ExpECEHistogram(num_bins=num_bins),\n",
    ")\n",
    "\n",
    "# strategy = DER(\n",
    "#     model,\n",
    "#     optimizer=None,\n",
    "#     device=device,\n",
    "#     evaluator=plugins,\n",
    "#     mem_size=2000,\n",
    "#     batch_size_mem=32,\n",
    "#     alpha=0.1,\n",
    "#     beta=0.5,\n",
    "#     eval_mb_size=32,\n",
    "# )\n",
    "\n",
    "strategy = Replay(\n",
    "    model,\n",
    "    optimizer=None,\n",
    "    criterion=CrossEntropyLoss(),\n",
    "    device=device,\n",
    "    evaluator=plugins,\n",
    "    mem_size=2000,\n",
    "    eval_mb_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for experience in range(len(test_stream)):\n",
    "    print(\"Computing experience\", experience)\n",
    "    print(\"Loading model:\", f\"{model_base_path}_exp{experience}.pt\")\n",
    "    strategy.model.load_state_dict(torch.load(f\"{model_base_path}_exp{experience}.pt\", map_location=device))\n",
    "    results.append(strategy.eval(benchmark.test_stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_accuracy = []\n",
    "running_ece = []\n",
    "final_accuracy = []\n",
    "final_ece = []\n",
    "bins = None\n",
    "ece_hist_vals = []\n",
    "mafia = []\n",
    "\n",
    "metric_str = \"Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp\"\n",
    "m = []\n",
    "for i in range(len(results)):\n",
    "    cur_exp_dict = results[i]\n",
    "    cur_exp_acc = 0\n",
    "    # compute the average over the experiences trained so far (i)\n",
    "    for j in range(i+1):\n",
    "        print(i, j, metric_str + f\"{j:03d}\", cur_exp_dict[metric_str + f\"{j:03d}\"])\n",
    "        cur_exp_acc += cur_exp_dict[metric_str + f\"{j:03d}\"]\n",
    "    m.append(cur_exp_acc/(i+1))\n",
    "\n",
    "# duplicate for JointTraining\n",
    "if len(m) < 10:\n",
    "    m = m*10\n",
    "running_accuracy.append(m)\n",
    "final_accuracy.append(running_accuracy[-1][-1])\n",
    "\n",
    "metric_str = \"ECE_Exp/eval_phase/test_stream/Task000/Exp\"\n",
    "m = []\n",
    "for i in range(len(results)):\n",
    "    cur_exp_dict = results[i]\n",
    "    cur_exp_ece = 0\n",
    "    # compute the average over the experiences trained so far (i)\n",
    "    for j in range(i+1):\n",
    "        print(i, j, metric_str + f\"{j:03d}\", cur_exp_dict[metric_str + f\"{j:03d}\"])\n",
    "        cur_exp_ece += cur_exp_dict[metric_str + f\"{j:03d}\"]\n",
    "        if i == j:\n",
    "            mafia.append(cur_exp_dict[metric_str + f\"{j:03d}\"])\n",
    "    # m.append(cur_exp_ece/(i+1))\n",
    "    m.append((cur_exp_ece/(i+1)))\n",
    "\n",
    "# duplicate for JointTraining\n",
    "if len(m) < 10:\n",
    "    m = m*10\n",
    "running_ece.append(m)\n",
    "final_ece.append(running_ece[-1][-1])\n",
    "# print(k, m, running_ece[-1][-1][-1])\n",
    "\n",
    "metric_str = \"ExpECEHistogram/eval_phase/test_stream/Exp\"\n",
    "m = []\n",
    "i = -1 # after last experience\n",
    "cur_exp_dict = results[i]\n",
    "for j in range(10):\n",
    "    print(i, j, metric_str + f\"{j:03d}\", cur_exp_dict[metric_str + f\"{j:03d}\"])\n",
    "    fig = cur_exp_dict[metric_str + f\"{j:03d}\"]\n",
    "    axes_list = fig.get_axes()\n",
    "    for ax in axes_list:\n",
    "        for line in ax.get_lines()[-1:]:\n",
    "            x_data = line.get_xdata()\n",
    "            y_data = line.get_ydata()\n",
    "            # print({'x': x_data, 'y': y_data})\n",
    "            if bins is None:\n",
    "                bins = x_data\n",
    "            m.append(y_data)\n",
    "# print(bins, m)\n",
    "bin_vals = []\n",
    "for i in range(len(bins)):\n",
    "    x = []\n",
    "    for j in range(len(m)):\n",
    "        # print(j, i, m[j][i])\n",
    "        x.append(m[j][i])\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    bin_vals.append((mean, std))\n",
    "# print(k, bin_vals)\n",
    "ece_hist_vals.append(bin_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 6))\n",
    "x_axis = list(range(1, 10+1))\n",
    "for i, vals in enumerate(running_accuracy):\n",
    "    plt.plot(x_axis, vals)\n",
    "plt.title('Average Experience Accuracy')\n",
    "plt.xlabel('#Trained Experience')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(1, 10)\n",
    "plt.xticks(x_axis, x_axis)\n",
    "# plt.legend(loc='upper right', fontsize='small', ncol=2)\n",
    "# plt.savefig(f'./imgs/{DATASET}_{NUM_EXPERIENCES}_{str.lower(\"Average_Experience_Accuracy\")}.png', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 6))\n",
    "x_axis = list(range(1, 10+1))\n",
    "for i, vals in enumerate(running_ece):\n",
    "    plt.plot(x_axis, vals)\n",
    "plt.title('Average Experience ECE')\n",
    "plt.xlabel('#Trained Experience')\n",
    "plt.ylabel('ECE')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(1, 10)\n",
    "plt.xticks(x_axis, x_axis)\n",
    "plt.grid(True, linestyle='--')\n",
    "# plt.legend(loc='upper right', fontsize='small', ncol=2)\n",
    "# plt.savefig(f'./imgs/{DATASET}_{NUM_EXPERIENCES}_{str.lower(\"Average_Experience_ECE\")}.png', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(1, 11)]\n",
    "plt.plot(x, mafia)\n",
    "plt.xlabel('#Trained Experience')\n",
    "plt.ylabel('ECE')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(1, 10)\n",
    "plt.xticks(x, x)\n",
    "plt.grid(True, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_accuracy, final_ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 4, figsize=(12, 8))\n",
    "axs = axs.flatten()\n",
    "for i, vals in enumerate(ece_hist_vals):\n",
    "    m = [e[0] for e in vals]\n",
    "    s = [e[1] for e in vals]\n",
    "    l = [max(e[0] - e[1], 0) for e in vals] # cap lower-bound at zero\n",
    "    u = [e[0] + e[1] for e in vals]\n",
    "    axs[i].plot([0, 1], [0, 1], '--', label='ideal')\n",
    "    # axs[i].plot(bins, m, color=valid_colors[i])\n",
    "    # axs[i].fill_between(bins, l, u, alpha=0.3, linestyle='--', color=valid_colors[i])\n",
    "    axs[i].errorbar(bins, m, yerr=s, marker=\"o\", linestyle=\"--\", capsize=3, capthick=1)\n",
    "    axs[i].set_ylim(-0.05, 1)\n",
    "    axs[i].set_xlim(0, 1)\n",
    "    axs[i].set_ylabel(\"Accuracy\")\n",
    "    axs[i].set_xlabel(\"Confidence\")\n",
    "    axs[i].grid(True, linestyle='--')\n",
    "    # axs[i].legend(loc='upper left', fontsize='small')\n",
    "    # axs[i].set_title(run2label[name])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'./imgs/{DATASET}_{NUM_EXPERIENCES}_avg_std_calibration.png', dpi=400)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calibration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
