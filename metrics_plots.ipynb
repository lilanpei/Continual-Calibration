{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./logs/run1\"\n",
    "DATASET = \"SplitMNIST\"\n",
    "MODEL_NAME = \"SimpleMLP\"\n",
    "NUM_EXPERIENCES = 5\n",
    "valid_colors = ['green', 'red', 'cyan', 'magenta', 'black', 'purple', 'orange', 'brown', 'gray', 'olive', 'indigo', 'turquoise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all combinations of training\n",
    "run2name = {\n",
    "    \"n_nst_npp\" : \"Naive_NoSelfTraining_NoPostProcessing\",\n",
    "    \"n_st_npp\" : \"Naive_SelfTraining_NoPostProcessing\",\n",
    "    \"n_nst_pp\" : \"Naive_NoSelfTraining_PostProcessing\",\n",
    "    \"n_nst_pp_md\" : \"Naive_NoSelfTraining_PostProcessing_MixedData\",\n",
    "\n",
    "    \"r_nst_npp\" : \"Replay_NoSelfTraining_NoPostProcessing\",\n",
    "    \"r_st_npp\" : \"Replay_SelfTraining_NoPostProcessing\",\n",
    "    \"r_nst_pp\" : \"Replay_NoSelfTraining_PostProcessing\",\n",
    "    \"r_nst_pp_md\" : \"Replay_NoSelfTraining_PostProcessing_MixedData\",\n",
    "    \n",
    "    \"j_nst_npp\" : \"JointTraining_NoSelfTraining_NoPostProcessing\",\n",
    "    \"j_st_npp\" : \"JointTraining_SelfTraining_NoPostProcessing\",\n",
    "    \"j_nst_pp\" : \"JointTraining_NoSelfTraining_PostProcessing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run2label = {\n",
    "    \"n_nst_npp\" : \"Naive\",\n",
    "    \"n_st_npp\" : \"Naive_SelfTraining\",\n",
    "    \"n_nst_pp\" : \"Naive_PostProcessing\",\n",
    "    \"n_nst_pp_md\" : \"Naive_PostProcessing_MixedData\",\n",
    "\n",
    "    \"r_nst_npp\" : \"Replay\",\n",
    "    \"r_st_npp\" : \"Replay_SelfTraining\",\n",
    "    \"r_nst_pp\" : \"Replay_PostProcessing\",\n",
    "    \"r_nst_pp_md\" : \"Replay_PostProcessing_MixedData\",\n",
    "    \n",
    "    \"j_nst_npp\" : \"JointTraining\",\n",
    "    \"j_st_npp\" : \"JointTraining_SelfTraining\",\n",
    "    \"j_nst_pp\" : \"JointTraining_PostProcessing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Naive_NoSelfTraining_NoPostProcessing <<\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './logs/run1/SplitMNIST_SimpleMLP_Naive_NoSelfTraining_NoPostProcessing_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, name \u001b[38;5;129;01min\u001b[39;00m run2name\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m <<\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATASET\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     11\u001b[0m         data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# print(\"\\n---- ACCURACY ----\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/continual-calibration/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './logs/run1/SplitMNIST_SimpleMLP_Naive_NoSelfTraining_NoPostProcessing_dict'"
     ]
    }
   ],
   "source": [
    "running_accuracy = []\n",
    "running_ece = []\n",
    "final_accuracy = []\n",
    "final_ece = []\n",
    "bins = None\n",
    "ece_hist_vals = []\n",
    "\n",
    "for k, name in run2name.items():\n",
    "    print(f\">> {name} <<\")\n",
    "    with open(f\"{DATA_PATH}/{DATASET}_{MODEL_NAME}_{name}_dict\", \"rb\") as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "        # print(\"\\n---- ACCURACY ----\")\n",
    "\n",
    "        metric_str = \"Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp\"\n",
    "        m = []\n",
    "        for i in range(len(data)):\n",
    "            cur_exp_dict = data[i]\n",
    "            cur_exp_acc = 0\n",
    "            # compute the average over the experiences trained so far (i)\n",
    "            for j in range(i+1):\n",
    "                # print(i, j, metric_str + f\"{j:03d}\", cur_exp_dict[metric_str + f\"{j:03d}\"])\n",
    "                cur_exp_acc += cur_exp_dict[metric_str + f\"{j:03d}\"]\n",
    "            m.append(cur_exp_acc/(i+1))\n",
    "        \n",
    "        # duplicate for JointTraining\n",
    "        if len(m) < NUM_EXPERIENCES:\n",
    "            m = m*NUM_EXPERIENCES\n",
    "        running_accuracy.append((k, m))\n",
    "        final_accuracy.append((k, running_accuracy[-1][-1][-1]))\n",
    "        # print(k, m, running_accuracy[-1][-1][-1])\n",
    "\n",
    "        # print(\"\\n---- ECE ----\")\n",
    "\n",
    "        metric_str = \"ECE_Exp/eval_phase/test_stream/Task000/Exp\"\n",
    "        m = []\n",
    "        for i in range(len(data)):\n",
    "            cur_exp_dict = data[i]\n",
    "            cur_exp_ece = 0\n",
    "            # compute the average over the experiences trained so far (i)\n",
    "            for j in range(i+1):\n",
    "                # print(i, j, metric_str + f\"{j:03d}\", cur_exp_dict[metric_str + f\"{j:03d}\"])\n",
    "                cur_exp_ece += cur_exp_dict[metric_str + f\"{j:03d}\"]\n",
    "            m.append(cur_exp_ece/(i+1))\n",
    "        \n",
    "        # duplicate for JointTraining\n",
    "        if len(m) < NUM_EXPERIENCES:\n",
    "            m = m*NUM_EXPERIENCES\n",
    "        running_ece.append((k, m))\n",
    "        final_ece.append((k, running_ece[-1][-1][-1]))\n",
    "        # print(k, m, running_ece[-1][-1][-1])\n",
    "\n",
    "        # print(\"\\n---- ECE HISTOGRAMS ----\")\n",
    "\n",
    "        metric_str = \"ExpECEHistogram/eval_phase/test_stream/Exp\"\n",
    "        m = []\n",
    "        i = -1 # after last experience\n",
    "        cur_exp_dict = data[i]\n",
    "        for j in range(NUM_EXPERIENCES):\n",
    "            # print(i, j, metric_str + f\"{j:03d}\", cur_exp_dict[metric_str + f\"{j:03d}\"])\n",
    "            fig = cur_exp_dict[metric_str + f\"{j:03d}\"]\n",
    "            axes_list = fig.get_axes()\n",
    "            for ax in axes_list:\n",
    "                for line in ax.get_lines()[-1:]:\n",
    "                    x_data = line.get_xdata()\n",
    "                    y_data = line.get_ydata()\n",
    "                    # print({'x': x_data, 'y': y_data})\n",
    "                    if bins is None:\n",
    "                        bins = x_data\n",
    "                    m.append(y_data)\n",
    "        # print(bins, m)\n",
    "        bin_vals = []\n",
    "        for i in range(len(bins)):\n",
    "            x = []\n",
    "            for j in range(len(m)):\n",
    "                # print(j, i, m[j][i])\n",
    "                x.append(m[j][i])\n",
    "            mean = np.mean(x)\n",
    "            std = np.std(x)\n",
    "            bin_vals.append((mean, std))\n",
    "        # print(k, bin_vals)\n",
    "        ece_hist_vals.append((k, bin_vals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 1: Average accuracy on all experiences after training on exp j\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "x_axis = list(range(NUM_EXPERIENCES))\n",
    "for i, (name, vals) in enumerate(running_accuracy):\n",
    "    plt.plot(x_axis, vals, label=run2label[name], color=valid_colors[i])\n",
    "plt.title('Average Experience Accuracy')\n",
    "plt.xlabel('#Trained Experience')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(0, NUM_EXPERIENCES-1)\n",
    "plt.xticks(x_axis, x_axis)\n",
    "plt.legend(loc='upper right', fontsize='small', ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 2: Average ece on all experiences after training on exp j\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "x_axis = list(range(NUM_EXPERIENCES))\n",
    "for i, (name, vals) in enumerate(running_ece):\n",
    "    plt.plot(x_axis, vals, label=run2label[name], color=valid_colors[i])\n",
    "plt.title('Average Experience ECE')\n",
    "plt.xlabel('#Trained Experience')\n",
    "plt.ylabel('ECE')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(0, NUM_EXPERIENCES-1)\n",
    "plt.xticks(x_axis, x_axis)\n",
    "plt.legend(loc='upper right', fontsize='small', ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE : average accuracy/ece on all experiences at the end of training\n",
    "\n",
    "table_data = []\n",
    "for (n, acc), (_, ece) in zip(final_accuracy, final_ece):\n",
    "    table_data.append((run2label[n], round(acc, 2), round(ece,2)))\n",
    "\n",
    "dt = pd.DataFrame(table_data, columns=[\"RunName\", \"Accuracy\", \"ECE\"])\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HISTOGRAM : avg/std across all experiences at the end of training\n",
    "\n",
    "fig, axs = plt.subplots(3, 4, figsize=(12, 8))\n",
    "axs = axs.flatten()\n",
    "for i, (name, vals) in enumerate(ece_hist_vals):\n",
    "    m = [e[0] for e in vals]\n",
    "    s = [e[1] for e in vals]\n",
    "    l = [max(e[0] - e[1], 0) for e in vals] # cap lower-bound at zero\n",
    "    u = [e[0] + e[1] for e in vals]\n",
    "    axs[i].plot([0, 1], [0, 1], '--', label='ideal')\n",
    "    # axs[i].plot(bins, m, color=valid_colors[i])\n",
    "    # axs[i].fill_between(bins, l, u, alpha=0.3, linestyle='--', color=valid_colors[i])\n",
    "    axs[i].errorbar(bins, m, yerr=s, marker=\"o\", linestyle=\"--\", capsize=3, capthick=1, color=valid_colors[i])\n",
    "    axs[i].set_ylim(-0.05, 1)\n",
    "    axs[i].set_ylabel(\"Accuracy\")\n",
    "    axs[i].set_xlabel(\"Confidence\")\n",
    "    # axs[i].legend(loc='upper left', fontsize='small')\n",
    "    axs[i].set_title(run2label[name])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calibration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
